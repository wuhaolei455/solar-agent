from typing import Mapping, Any, cast
from langchain_core.messages import AIMessageChunk, BaseMessageChunk
from langchain_openai.chat_models import base

_original_convert = base._convert_delta_to_message_chunk


def _patched_convert(
    _dict: Mapping[str, Any], default_class: type[BaseMessageChunk]
) -> BaseMessageChunk:
    chunk = _original_convert(_dict, default_class)
    try:
        role = cast(str, _dict.get("role"))
        if _dict.get("reasoning_content") and (
            role == "assistant" or default_class == AIMessageChunk
        ):
            chunk.additional_kwargs["reasoning_content"] = _dict["reasoning_content"]
    except Exception:
        pass
    return chunk


base._convert_delta_to_message_chunk = _patched_convert

########################  hook  ########################

import os
from shared import setup

import gradio as gr
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessageChunk
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

setup()

store = {}


def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


english_tutor_prompt = ChatPromptTemplate.from_messages([
    ("system", """You are a friendly English tutor.

        Help the learner improve step by step.
        Keep responses short, clear, and conversational.

        When correcting:
        - First show the corrected sentence.
        - Then give a very brief reason (1â€“2 short sentences).
        - Use simple, everyday English.
        - Encourage the learner to try again.

        Do not give long explanations or grammar lectures.
    """),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{user_message}"),
])


def build_chain(deep_thinking: bool):
    """æ ¹æ®æ˜¯å¦å¼€å¯æ·±åº¦æ€è€ƒï¼Œæ„å»ºä¸åŒçš„ chainã€‚

    - æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨ .env ä¸­é…ç½®çš„æ¨¡å‹ï¼Œç›´æ¥å›ç­”
    - æ·±åº¦æ€è€ƒï¼šä½¿ç”¨ deepseek-v3.2-thinkï¼Œè¿”å›æ€è€ƒè¿‡ç¨‹ + å›ç­”
    """
    if deep_thinking:
        print("[Deep Thinking] å·²å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼ (deepseek-v3.2-think)")
        model = init_chat_model(
            "openai:deepseek-v3.2-think",
            temperature=0.6,
        )
    else:
        model = init_chat_model(
            "openai:gpt-5.2",
            temperature=0
        )

    # æ³¨æ„ï¼šä¸åŠ  StrOutputParserï¼Œä¿ç•™åŸå§‹ AIMessageChunk ä»¥è¯»å– reasoning_content
    chain = english_tutor_prompt | model

    return RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="user_message",
        history_messages_key="chat_history",
    )


def stream_ai_response(user_message: str, session_id: str, deep_thinking: bool):
    """æµå¼è°ƒç”¨å¤§æ¨¡å‹ï¼Œåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›ç­”ã€‚"""
    chain_with_history = build_chain(deep_thinking)

    thinking_buffer = ""
    answer_buffer = ""

    for chunk in chain_with_history.stream(
        {"user_message": user_message},
        config={"configurable": {"session_id": session_id}}
    ):
        if not isinstance(chunk, AIMessageChunk):
            continue

        # æ€è€ƒè¿‡ç¨‹ï¼ˆä»…å±•ç¤ºï¼Œä¸å­˜å…¥å¯¹è¯è®°å¿†ï¼‰
        if "reasoning_content" in chunk.additional_kwargs:
            thinking_buffer += chunk.additional_kwargs["reasoning_content"]

        # æœ€ç»ˆå›ç­”ï¼ˆè‡ªåŠ¨å­˜å…¥å¯¹è¯è®°å¿†ï¼‰
        if chunk.content:
            answer_buffer += chunk.content

        # ç»„è£…è¾“å‡ºï¼šæœ‰æ€è€ƒæ—¶æ˜¾ç¤ºæ€è€ƒå—ï¼Œå§‹ç»ˆæ˜¾ç¤ºå›ç­”
        if thinking_buffer:
            yield f"<details><summary>ğŸ’­ æ€è€ƒè¿‡ç¨‹</summary>\n\n{thinking_buffer}\n\n</details>\n\n{answer_buffer}"
        else:
            yield answer_buffer


def chat_handler(message: str, history: list, deep_thinking: bool):
    session_id = "user_001"
    for partial in stream_ai_response(message, session_id, deep_thinking):
        yield partial


chat_ui = gr.ChatInterface(
    fn=chat_handler,
    additional_inputs=[
        gr.Checkbox(label="ğŸ§  æ·±åº¦æ€è€ƒ", value=False)
    ],
    title="è‹±è¯­å­¦ä¹ åŠ©æ‰‹ï¼ˆæ·±åº¦æ€è€ƒç‰ˆï¼‰",
    description="æ”¯æŒæ™®é€šæ¨¡å¼ / æ·±åº¦æ€è€ƒæ¨¡å¼çš„è‹±è¯­å­¦ä¹ åŠ©æ‰‹ã€‚å‹¾é€‰ã€Œæ·±åº¦æ€è€ƒã€å¯æŸ¥çœ‹æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚"
)


if __name__ == "__main__":
    os.environ.setdefault("no_proxy", "localhost,127.0.0.1")
    chat_ui.launch(server_name="127.0.0.1", server_port=7880, share=False)
